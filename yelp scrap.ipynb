{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.yelp.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yelp_scrap(service, location, pages):\n",
    "    restaurant_name=[]\n",
    "    phone=[]\n",
    "    address=[]\n",
    "    ratings=[]\n",
    "    links=[]\n",
    "    pg=10\n",
    "    driver.get(\"https://www.yelp.com/\")\n",
    "    driver.find_element(By.NAME, \"find_desc\").send_keys(service+Keys.ENTER)\n",
    "    driver.find_element(By.ID, \"search_location\").send_keys(Keys.CONTROL, \"a\")\n",
    "    driver.find_element(By.ID, \"search_location\").send_keys(Keys.DELETE)\n",
    "    driver.find_element(By.ID, \"search_location\").send_keys(location+Keys.ENTER)\n",
    "    time.sleep(2)\n",
    "    soup=BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    a=soup.find(\"div\", {\"role\": \"navigation\"})\n",
    "    b=int(a.text.split(\" \")[-1])\n",
    "    if pages>b:\n",
    "        print(f\"There are only {b} pages present on the webpage. Scrapping {b} pages!\")\n",
    "        pages=b\n",
    "    site=driver.current_url\n",
    "    for z in range(0,pages*10,10):\n",
    "        c=[]\n",
    "        elements=[]\n",
    "        baseurl=\"https://www.yelp.com\"\n",
    "        driver.get(f\"{site}&start={z}\")\n",
    "        time.sleep(1)\n",
    "        soup=BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        for i in soup.find_all(\"a\",{\"class\":\"css-19v1rkv\"}):\n",
    "            c.append(i)\n",
    "        c=c[2:]\n",
    "        for i in c:\n",
    "            if len(str(i))<270:\n",
    "                elements.append(str(i))\n",
    "        for i in elements:\n",
    "            i=i.split(\" \")\n",
    "            d=i[2]\n",
    "            if d[0]==\"h\":\n",
    "                links.append(f\"{baseurl}{d[6:-1]}\")\n",
    "    for x in links:\n",
    "        driver.get(x)\n",
    "        soup=BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        restaurant_name.append(soup.find(\"h1\", {\"class\": \"css-1se8maq\"}).text)\n",
    "        phone.append(driver.find_elements(By.XPATH, '//p[@class=\" css-1p9ibgf\"][1]')[1].text)\n",
    "        address.append(driver.find_element(By.XPATH, '//p[@class=\" css-qyp8bo\"]').text)\n",
    "    df=pd.DataFrame({\"Restaurant_Name\": restaurant_name, \"Phone_Number\": phone, \"Address\": address})\n",
    "    df.to_excel(\"Yelp Scrap Data.xlsx\", sheet_name=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service=input(f\"Kindly enter the name of the service required: \")\n",
    "locality=input(f\"Kindly enter the name of the locality: \")\n",
    "pag=int(input(\"Kindly enter the number of pages to be scrapped: \"))\n",
    "yelp_scrap(service, locality, pag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
